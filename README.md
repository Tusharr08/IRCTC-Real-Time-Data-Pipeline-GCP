# üöÑ IRCTC Streaming Data Ingestion Project

## All Aboard! The Data Express is Leaving the Station...

Welcome to a project that brings the hustle and bustle of India's railway system into the world of real-time data analytics. This project simulates the immense flow of data generated by IRCTC, the Indian Railway Catering and Tourism Corporation, and channels it through a robust, scalable, and secure data pipeline on Google Cloud Platform (GCP).

### üßê What's the Big Idea?

Imagine a million passengers booking tickets, a thousand cancellations happening every minute, and countless train updates‚Äîall happening simultaneously. This project builds a production-grade **real-time ETL pipeline** to handle this chaos gracefully. We don't just move data; we validate, transform, and securely land it in a data warehouse, ready for instant analysis.

### üõ†Ô∏è The Tech Stack: Engineering the Railway

This pipeline is powered by a suite of cutting-edge GCP services and modern data engineering tools:

* **Python:** The engine room of our project, powering our data simulation and our pipeline code.
* **GCP Pub/Sub:** The high-speed rail tracks, providing a fully managed, scalable messaging service to ingest events as they happen.
* **Apache Beam / GCP Dataflow:** The intelligent train conductor, orchestrating the ETL (Extract, Transform, Load) process. Dataflow provides a serverless, autoscaling framework to ensure our data gets to its destination efficiently, no matter the volume.
* **GCP BigQuery:** The central railway hub, a serverless, highly scalable data warehouse where our streaming data arrives, ready for SQL queries.
* **GCP Cloud KMS (Key Management Service):** The Fort Knox of our project. We use Customer-Managed Encryption Keys (CMEK) to ensure our data is encrypted at rest in BigQuery, meeting stringent security and compliance standards.

### üó∫Ô∏è The Journey: A Glimpse into the Pipeline

1.  **Departure (Source):** A Python script simulates IRCTC booking and cancellation events and publishes them to a **Pub/Sub topic**.
2.  **On the Tracks (Processing):** An Apache Beam pipeline, executed by **Dataflow**, reads these events in real time. It cleans and validates the data to ensure quality.
3.  **Secure Arrival (Destination):** The transformed data is loaded into a table in **BigQuery**, where it is automatically encrypted using a key managed by **Cloud KMS**.

### üí° Why this Matters

This project isn't just about moving data; it's a showcase of critical data engineering skills:

* **Streaming ETL:** Demonstrates expertise in building real-time data pipelines.
* **GCP Proficiency:** Hands-on experience with core GCP services like Pub/Sub, Dataflow, and BigQuery.
* **Data Security:** Implementation of a robust encryption strategy using CMEK and KMS, a key requirement for modern data platforms.
* **Scalability:** A solution designed to handle massive volumes of data with serverless and autoscaling technologies.

### üöÄ Get Started

Want to see the data express in action?

1.  Clone this repository.
2.  Set up your GCP environment by enabling the required APIs.
3.  Run the producer script to start the data flow.
4.  Launch the Dataflow pipeline to see the data arrive in BigQuery in real-time.

---

_This project is for educational purposes and uses simulated data. It is not affiliated with the official IRCTC._
